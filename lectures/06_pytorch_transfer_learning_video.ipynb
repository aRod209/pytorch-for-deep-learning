{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXE8YEG52T0JPGmOfQPjoZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aRod209/pytorch-for-deep-learning/blob/main/lectures/06_pytorch_transfer_learning_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning\n",
        "\n",
        "What is transfer learning?\n",
        "\n",
        "Transfer learning involves taking the parameters of what one model has learned on another dataset and applying to our own problem.\n",
        "\n",
        "* Pretrained model = foundation model"
      ],
      "metadata": {
        "id": "0bWRxfGUgdTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-zkqBXSgXQY",
        "outputId": "4a375d02-1970-4a0d-a093-e654fc1aad98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n",
            "0.17.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "# I know we have versions that are greater than these versions, but I just\n",
        "# want to run the below code anyways to know how to update to targeted versions.\n",
        "\n",
        "try:\n",
        "  assert int(torch.__version__.split('.')[0]) >= 1, 'torch version should be at least version 1'\n",
        "\n",
        "  if int(torch.__version__.split('.')[0]) == 1:\n",
        "    assert int(torch.__version__.split('.')[1]) >= 12, 'torch version should be 1.12+'\n",
        "\n",
        "  if int(torchvision.__version__.split('.')[0]) == 0:\n",
        "    assert int(torchvision.__version__.split('.')[1]) >= 13, 'torchvision version should be 0.13+'\n",
        "except:\n",
        "  print(f'[INFO] torch/torchvision version not as required, installing nightly versions.')\n",
        "  !pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "finally:\n",
        "  print(f'torch version: {torch.__version__}')\n",
        "  print(f'torchvision version: {torchvision.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH-I4PBuhY-m",
        "outputId": "a25da0d5-a6e7-46c4-e31a-14d7d3327461"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.2.1+cu121\n",
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got the versions of `torch` and `torchvision` we're after, let's import the code we've written in previous sections so that we don't have to write it all again."
      ],
      "metadata": {
        "id": "IJtDuSLAialE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to get torchinfo, installit if it does not work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q  torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# Try to import the going modular directory, download it from GitHub if it does not work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] Couldn't find foing_modular scripts... downloading them from GitHub.\")\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "n7jaD9VRi1pB",
        "outputId": "a2f826dd-e833-49a3-ec1f-af32c5bb7d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find foing_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Total 4056 (delta 0), reused 0 (delta 0), pack-reused 4056\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 646.90 MiB | 36.93 MiB/s, done.\n",
            "Resolving deltas: 100% (2371/2371), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "fjOUoD5kmqY1",
        "outputId": "922dfe1f-272b-45c6-c41e-c2d8ff1a6079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "1pit-Nb4m7dI",
        "outputId": "c89395a6-aaaf-43cb-909b-88284b87c84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data\n",
        "\n",
        "We need our pzza, steak, sushi data to build a transfer learning model on."
      ],
      "metadata": {
        "id": "BZaqj79hm9bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup data path\n",
        "data_path = Path('data/')\n",
        "image_path = data_path / 'pizza_steak_sushi' # images from a subset of classes from the Food101 dataset\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f'{image_path} directory exists, skipping re-download')\n",
        "else:\n",
        "  print(f'Did not find {image_path}, downloading it...')\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Download pizza, steak, sushi data\n",
        "  with open(data_path / 'pizza_steak_sushi.zip', 'wb') as f:\n",
        "    response = requests.get('https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip')\n",
        "    print('Downloading pizza, steak, sushi, data...')\n",
        "    f.write(response.content)\n",
        "\n",
        "    # unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / 'pizza_steak_sushi.zip', 'r') as zip_ref:\n",
        "      print('Unzipping pizza, steak, sushi data...')\n",
        "      zip_ref.extractall(image_path)\n",
        "\n",
        "      # Remove .zip file\n",
        "      os.remove(data_path / 'pizza_steak_sushi.zip')"
      ],
      "metadata": {
        "id": "vW4STpEu3wI6",
        "outputId": "253d206a-3100-413b-bf88-f2976bf7eef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi, downloading it...\n",
            "Downloading pizza, steak, sushi, data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory path\n",
        "train_dir = image_path / 'train'\n",
        "test_dir = image_path / 'test'\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "EBhKA42o5yLi",
        "outputId": "68ada7cf-2b9b-4976-ef14-d1fea96957e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Now we've got some data. Now we want to turn it into PyTorch `DataLoader`s\n",
        "\n",
        "To do so, we can use `data_setup.py` and the `create_dataloaders()` function we maid in 05. PyTorch Going Modular\n",
        "\n",
        "There's one thing we have to think about when loading: how to **transform** it?\n",
        "\n",
        "And with `torchvision` 0.13+ there's two ways to do this:\n",
        "\n",
        "1. Manually created transforms - you define what tranforms you want your data to go through\n",
        "2. Automatically created transforms - the transforms for your dat are defined by the model you'd like to use.\n",
        "\n",
        "Important point: when using a pretrained model, it's important that your data (including your custom data) that yo pass through it is **transformed** in the same way that the data the model was trained on.\n"
      ],
      "metadata": {
        "id": "FY2opncs55eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "`torchvision.models` contains pretrained models (models ready for transfer learning) right within `torchvision`.\n",
        "\n",
        ">All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. You can use the following transform to normalize:"
      ],
      "metadata": {
        "id": "ZMj7NGHaYXLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # resize image to 224, 224 (height x width)\n",
        "    transforms.ToTensor(), # get images into range [0, 1]\n",
        "    normalize # make sure images have the same distribution as ImageNet (where our pretrained models have been trained)\n",
        "    ])"
      ],
      "metadata": {
        "id": "ByveNH8VcNbx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms,\n",
        "                                                                               batch_size=32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "5dUi8OfPdeWW",
        "outputId": "4c7564fe-b4e5-4ee7-c185-97b74e118332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7d647d1d82b0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7d647d1d9d80>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vnhquP5d0lK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}